{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNikEuJesFmZ/uI2qalbKvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17f501bf3487414baf8f64e14e7890a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_824265d9236545f7a0503236ee7d1d27",
              "IPY_MODEL_8196177a52d445bebcc7d4f3ff8e0703",
              "IPY_MODEL_7aadcf8dc82144c5a81b10f617b8686a"
            ],
            "layout": "IPY_MODEL_016fb6b5b73f4ecf84ecfdd5d228d3b7"
          }
        },
        "824265d9236545f7a0503236ee7d1d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2501fb2b5a2843d2858f32f3ec261a9e",
            "placeholder": "​",
            "style": "IPY_MODEL_7b120ecf7f3f4535a954b2e30db4d980",
            "value": "  0%"
          }
        },
        "8196177a52d445bebcc7d4f3ff8e0703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a22ad9534c040a28c10522d04f728da",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_351700ee15094fb1835fc3c7f896e35e",
            "value": 0
          }
        },
        "7aadcf8dc82144c5a81b10f617b8686a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55eab2df550b46a8bb215cee6ed37967",
            "placeholder": "​",
            "style": "IPY_MODEL_026d9483f5de471cbd800e19dc658892",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "016fb6b5b73f4ecf84ecfdd5d228d3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2501fb2b5a2843d2858f32f3ec261a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b120ecf7f3f4535a954b2e30db4d980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a22ad9534c040a28c10522d04f728da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351700ee15094fb1835fc3c7f896e35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55eab2df550b46a8bb215cee6ed37967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026d9483f5de471cbd800e19dc658892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moosunny/Transformer_Translator/blob/main/Seq2Seq_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4qzFD5FVerS",
        "outputId": "72ae6749-311d-4a60-fe62-ffbc214215a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "LixDt3ZkVHtV",
        "outputId": "25e07f6a-4673-4df0-d7bf-4a04328530d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         대분류 소분류                            상황  Set Nr.  발화자  \\\n",
              "0       비즈니스  회의                       의견 교환하기        1  A-1   \n",
              "1       비즈니스  회의                       의견 교환하기        1  B-1   \n",
              "2       비즈니스  회의                       의견 교환하기        1  A-2   \n",
              "3       비즈니스  회의                       의견 교환하기        1  B-2   \n",
              "4       비즈니스  회의                       의견 교환하기        2  A-1   \n",
              "...      ...  ..                           ...      ...  ...   \n",
              "99995  여행/쇼핑  쇼핑  계산/포장/배달 (계산 장소 문의, 계산 오류 등)    24999  B-2   \n",
              "99996  여행/쇼핑  쇼핑  계산/포장/배달 (계산 장소 문의, 계산 오류 등)    25000  A-1   \n",
              "99997  여행/쇼핑  쇼핑  계산/포장/배달 (계산 장소 문의, 계산 오류 등)    25000  B-1   \n",
              "99998  여행/쇼핑  쇼핑  계산/포장/배달 (계산 장소 문의, 계산 오류 등)    25000  A-2   \n",
              "99999  여행/쇼핑  쇼핑  계산/포장/배달 (계산 장소 문의, 계산 오류 등)    25000  B-2   \n",
              "\n",
              "                                             원문  \\\n",
              "0                   이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
              "1                    판매량이 지난번 제품보다 빠르게 늘고 있습니다.   \n",
              "2                  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.   \n",
              "3                   네, 제가 연락해서 주문량을 2배로 늘리겠습니다.   \n",
              "4                   지난 회의 마지막에 논의했던 안건을 다시 볼까요?   \n",
              "...                                         ...   \n",
              "99995        저희가 가격표 배치를 잘못해서 혼동을 드렸나 봐요, 죄송해요.   \n",
              "99996                 백화점 포인트로 계산하고 싶은데, 가능한가요?   \n",
              "99997                 네, 물론이죠, 전화번호 입력해주시면 됩니다.   \n",
              "99998              입력했어요, 전액 백화점 포인트로 결제하고 싶어요.   \n",
              "99999  죄송하지만 포인트 제외한 차액 15,000원은 따로 결제해주셔야 합니다.   \n",
              "\n",
              "                                                     번역문  \n",
              "0      How is the market's reaction to the newly rele...  \n",
              "1      The sales increase is faster than the previous...  \n",
              "2      Then, we'll have to call the manufacturer and ...  \n",
              "3      Sure, I'll make a call and double the volume o...  \n",
              "4      Shall we take a look at the issues we discusse...  \n",
              "...                                                  ...  \n",
              "99995  It seems that we didn't place the price tags c...  \n",
              "99996       Can I pay using the department store points?  \n",
              "99997  Yes, of course, you just need to enter your ph...  \n",
              "99998  I entered it, I want to pay it with all the de...  \n",
              "99999  I'm sorry, but you need to make a separate pay...  \n",
              "\n",
              "[100000 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a26f9f29-667d-4b6d-ad7c-20f03d379b43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>대분류</th>\n",
              "      <th>소분류</th>\n",
              "      <th>상황</th>\n",
              "      <th>Set Nr.</th>\n",
              "      <th>발화자</th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>비즈니스</td>\n",
              "      <td>회의</td>\n",
              "      <td>의견 교환하기</td>\n",
              "      <td>1</td>\n",
              "      <td>A-1</td>\n",
              "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
              "      <td>How is the market's reaction to the newly rele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>비즈니스</td>\n",
              "      <td>회의</td>\n",
              "      <td>의견 교환하기</td>\n",
              "      <td>1</td>\n",
              "      <td>B-1</td>\n",
              "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다.</td>\n",
              "      <td>The sales increase is faster than the previous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>비즈니스</td>\n",
              "      <td>회의</td>\n",
              "      <td>의견 교환하기</td>\n",
              "      <td>1</td>\n",
              "      <td>A-2</td>\n",
              "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.</td>\n",
              "      <td>Then, we'll have to call the manufacturer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>비즈니스</td>\n",
              "      <td>회의</td>\n",
              "      <td>의견 교환하기</td>\n",
              "      <td>1</td>\n",
              "      <td>B-2</td>\n",
              "      <td>네, 제가 연락해서 주문량을 2배로 늘리겠습니다.</td>\n",
              "      <td>Sure, I'll make a call and double the volume o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>비즈니스</td>\n",
              "      <td>회의</td>\n",
              "      <td>의견 교환하기</td>\n",
              "      <td>2</td>\n",
              "      <td>A-1</td>\n",
              "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요?</td>\n",
              "      <td>Shall we take a look at the issues we discusse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>여행/쇼핑</td>\n",
              "      <td>쇼핑</td>\n",
              "      <td>계산/포장/배달 (계산 장소 문의, 계산 오류 등)</td>\n",
              "      <td>24999</td>\n",
              "      <td>B-2</td>\n",
              "      <td>저희가 가격표 배치를 잘못해서 혼동을 드렸나 봐요, 죄송해요.</td>\n",
              "      <td>It seems that we didn't place the price tags c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>여행/쇼핑</td>\n",
              "      <td>쇼핑</td>\n",
              "      <td>계산/포장/배달 (계산 장소 문의, 계산 오류 등)</td>\n",
              "      <td>25000</td>\n",
              "      <td>A-1</td>\n",
              "      <td>백화점 포인트로 계산하고 싶은데, 가능한가요?</td>\n",
              "      <td>Can I pay using the department store points?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>여행/쇼핑</td>\n",
              "      <td>쇼핑</td>\n",
              "      <td>계산/포장/배달 (계산 장소 문의, 계산 오류 등)</td>\n",
              "      <td>25000</td>\n",
              "      <td>B-1</td>\n",
              "      <td>네, 물론이죠, 전화번호 입력해주시면 됩니다.</td>\n",
              "      <td>Yes, of course, you just need to enter your ph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>여행/쇼핑</td>\n",
              "      <td>쇼핑</td>\n",
              "      <td>계산/포장/배달 (계산 장소 문의, 계산 오류 등)</td>\n",
              "      <td>25000</td>\n",
              "      <td>A-2</td>\n",
              "      <td>입력했어요, 전액 백화점 포인트로 결제하고 싶어요.</td>\n",
              "      <td>I entered it, I want to pay it with all the de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>여행/쇼핑</td>\n",
              "      <td>쇼핑</td>\n",
              "      <td>계산/포장/배달 (계산 장소 문의, 계산 오류 등)</td>\n",
              "      <td>25000</td>\n",
              "      <td>B-2</td>\n",
              "      <td>죄송하지만 포인트 제외한 차액 15,000원은 따로 결제해주셔야 합니다.</td>\n",
              "      <td>I'm sorry, but you need to make a separate pay...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a26f9f29-667d-4b6d-ad7c-20f03d379b43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a26f9f29-667d-4b6d-ad7c-20f03d379b43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a26f9f29-667d-4b6d-ad7c-20f03d379b43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c9acdaf6-ae97-4e94-bc95-a77c002829bb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9acdaf6-ae97-4e94-bc95-a77c002829bb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c9acdaf6-ae97-4e94-bc95-a77c002829bb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_56ce59d7-5106-4e7f-9e86-e19f042793f9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_56ce59d7-5106-4e7f-9e86-e19f042793f9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"\\ub300\\ubd84\\ub958\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\uc77c\\uc0c1\\ub300\\ud654\",\n          \"\\uc5ec\\ud589/\\uc1fc\\ud551\",\n          \"\\uc758\\ud559\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc18c\\ubd84\\ub958\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"\\ud68c\\uc758\",\n          \"\\uc778\\uc0ac\",\n          \"\\ubd80\\uc11c\\ubcc4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc0c1\\ud669\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2779,\n        \"samples\": [\n          \"\\ubc30\\uc1a1 \\ub3c4\\ucc29 \\uc77c\\uc790\\uac00 \\uad81\\uae08\\ud55c \\uc0c1\\ud669\",\n          \"\\uc1a1\\ubcc4\\ud68c\\uac00 \\uc5b8\\uc81c\\uc778\\uc9c0 \\ubb3c\\uc5b4\\ubcf4\\ub294 \\uc0c1\\ud669\",\n          \"\\ud654\\uc7ac \\uc0c1\\ud669, \\uc2e0\\uace0, \\uae30\\ud0c0 \\uc7ac\\ub09c \\uc0c1\\ud669 (\\ud0dc\\ud48d, \\ud64d\\uc218, \\uc9c0\\uc9c4 \\ub4f1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Set Nr.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7216,\n        \"min\": 1,\n        \"max\": 25000,\n        \"num_unique_values\": 25000,\n        \"samples\": [\n          6869,\n          24017,\n          9669\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ubc1c\\ud654\\uc790\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"B-1\",\n          \"B-2\",\n          \"A-1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc6d0\\ubb38\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99987,\n        \"samples\": [\n          \"3\\uc77c \\ub4a4\\uba74 \\uad1c\\ucc2e\\uc740\\ub370, \\uba87 \\ubc15 \\uba70\\uce60\\ub85c \\uac00\\ub294 \\uc0c1\\ud488\\uc778\\uac00\\uc694?\",\n          \"\\ub124, \\uc5b4\\uc81c \\uc774 \\uc2e0\\uc6a9\\uce74\\ub4dc\\ub85c \\uacb0\\uc81c\\ud588\\uc5c8\\uc2b5\\ub2c8\\ub2e4.\",\n          \"\\uad6c\\ub9e4\\ud558\\uc2e0 \\uc9c0 \\ud558\\ub8e8\\ubc16\\uc5d0 \\uc9c0\\ub098\\uc9c0 \\uc54a\\uc558\\ub2e4\\uba74 \\ubb3c\\ub860 \\uac00\\ub2a5\\ud569\\ub2c8\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ubc88\\uc5ed\\ubb38\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99916,\n        \"samples\": [\n          \"If the company understands the circumstances, the contract is terminated immediately.\",\n          \"You used a coupon and got a 10 percent discount.\",\n          \"You are right, so please take a coffee inside of my bag and hand it to me.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from transformers import MarianMTModel, MarianTokenizer, AutoTokenizer # MT: Machine Translation\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import math, random\n",
        "\n",
        "# 한국어 토크나이저 활용(upstage)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")\n",
        "# 패딩을 오른쪽으로 설정\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "# 한국어-영어 대화체 데이터 다운로드(link:)\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Transformer_Translation/2_대화체.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 모음\n",
        "max_len = 100\n",
        "batch_size = 128\n",
        "vocab_size = tokenizer.vocab_size # 64000개"
      ],
      "metadata": {
        "id": "z3cRxKkO9nL1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data.loc[idx, \"원문\"], self.data.loc[idx, \"번역문\"]\n",
        "\n",
        "custom_DS = CustomDataset(data)\n",
        "print(custom_DS[0])\n",
        "\n",
        "train_DS, val_DS, test_DS = torch.utils.data.random_split(custom_DS, [97000, 2000, 1000])\n",
        "# 논문에서는 450만개 영,독 문장 pair 사용\n",
        "\n",
        "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=batch_size, shuffle=True)\n",
        "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=batch_size, shuffle=True)\n",
        "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_DS))\n",
        "print(len(val_DS))\n",
        "print(len(test_DS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCEn8KbThBH1",
        "outputId": "fd95a7f9-6451-4737-b5ff-e7ddaf7f3931"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('이번 신제품 출시에 대한 시장의 반응은 어떤가요?', \"How is the market's reaction to the newly released product?\")\n",
            "97000\n",
            "2000\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5\n",
        "src_text, trg_text  = test_DS[0]\n",
        "print(f\"인덱스:{test_DS.indices[i]}\")\n",
        "print(f\"원문:{src_text}\")\n",
        "print(f\"원문:{trg_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPXotOX57cST",
        "outputId": "4713bfc8-dcc8-4b05-fc12-16f857f38ee9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스:57851\n",
            "원문:그럼 한 번에 몇 개까지 만들어서 판매해야 할까요?\n",
            "원문:Then, how many at a time should I make and sell?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qkg0V45UztAN",
        "outputId": "70a20304-1cb0-48a0-8429-a7755024ce60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eos_idx = tokenizer.eos_token_ids\n",
        "print(eos_idx)\n",
        "\n",
        "for src_text, trg_text in train_DL:\n",
        "  print(src_text)\n",
        "  print(trg_text)\n",
        "  print(len(src_text))\n",
        "  print(len(trg_text))\n",
        "\n",
        "  src = tokenizer(src_text, padding = True, truncation=True, max_length=max_len, return_tensors = 'pt', add_special_tokens = False).input_ids\n",
        "  trg_text = [tokenizer.bos_token + s for s in trg_text]\n",
        "  trg = tokenizer(trg_text, padding = True, truncation=True, max_length=max_len, return_tensors = 'pt', add_special_tokens = False).input_ids\n",
        "\n",
        "  print(src[:2])\n",
        "  print(trg[:2])\n",
        "  print(src.shape)\n",
        "  print(trg.shape)\n",
        "\n",
        "  print(trg[:, -1])\n",
        "  print(tokenizer.decode(trg[trg[:,-1]==eos_idx,:][0]))\n",
        "  print(trg[5, :-1])\n",
        "  print(trg[5, 1:])\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "id": "U0zMRvclxWpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dd5ea9-23ec-4b6e-ae0b-430d673fee4a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "('조부모상 같은 경우에는 총 4일 사용 가능하시고 연결해서 사용하셔야 해요.', '케이크도 식품인데, 왜 여기서는 안 파는 건가요?', '죄송하지만, 고객님의 성함과 연락처 확인 부탁드리겠습니다.', '그러게, 여기가 어디인지를 알아야 찾아갈 텐데.', '여기가 이영자가 나와서 소개했던 그 휴게소구나.', '나도 오늘 알게 됐는데 이번에 20번 버스 노선이 확장됐더라.', '정기 회원권을 구매하고 싶어요.', '두려워도 도전해보는 게 제일 중요한 것 같아.', '차량에 흠집이나 결점 같은 걸 기록해 두려고 그러는 거지?', '나는 스노보딩 하프파이프가 제일 보고 싶어.', '다음에 내가 여행을 가게 되면 너한테 좀 환전을 해야겠네.', '고속도로로 가면 요금이 더 추가되는데 괜찮으세요?', '지하 1층으로 내려가면 식품 판매대 안에 있을 거예요.', '그건 동감해, 간격이 좁아서 팔걸이가 없으니 너무 불편하더라.', '기내 면세품에 대한 안내는 앞에 있는 책자를 보시면 됩니다.', '어제 인사팀에서 월례회의할 때 미팅룸 B에서 사용했습니다.', '감사합니다. 혹시 프린터도 있나요?', '네. 이번 시험은 100페이지부터 250페이지까지입니다.', '정말 죄송합니다, 새 제품으로 교환해드리겠습니다.', '제가 어릴 적부터 이탈리아에 있어서 억양이 다른 사람들과 다를 거예요.', '지금 원화로 바꾸면 손해가 큽니다. 급하게 원화가 필요하신 게 아니라면 가지고 있는 게 좋을 것 같습니다.', '저기 경찰서에 가서 한 번 여쭤보는 게 좋을 것 같아.', '그 많은 학생을 어떻게 관리하나 걱정했는데 천만다행입니다.', '저희야말로 이렇게 좋은 식당에서 대접을 받아서 감사하죠.', '네, 2잔 모두 이 도장에 찍으면 되나요?', '네 그 전 회사에서 그 정도 받고 일을 해왔습니다.', '영웅물 영화가 영화관을 다 차지해서 선택권이 너무 없어.', '네. 사무실이 넓어서 3개 정도 신청해야 할 것 같아요.', '수납하려는데요, 여기서 수납하는 게 맞는 건가요?', '내가 꼬마 때부터 봤어도 야구 규칙은 정말 어려운 게 많아.', '네, 같은 색상과 디자인으로 사이즈만 90으로 해서 바로 배송해드리겠습니다.', '요 며칠 전부터 자꾸 무릎이 쑤셔서 계단을 사용할 수가 없어요.', '저는 프로그램 개발만 수년 동안 공부했으며, 이번에 지원한 개발팀에 필요하리라 생각합니다.', '다 해서 총 12,000원인데 어떤 것으로 결제를 도와드릴까요?', '독채는 온돌방으로 되어 있나요?', '혹시 모르니까 오늘 안에는 반드시 입력되도록 다시한번 요청해주세요.', '죄송합니다, 조리과정에서 실수가 있었나 봅니다.', '그렇습니다, 정가는 120,000원이며 정말 싸게 사시는 거예요.', '충분히 만들 수 있을 것 같아, 지금 운동 시작했어?', '이곳은 여름에 가야 정말 예쁘대.', '우리 집 바로 건너편인 401호에 맡겨주셨으면 좋겠어요.', '교양이라 상관없으니까 제발 도와줘.', '제 생각에 동양인은 잘 쓰지 않을 것 같은 색깔인데요.', '저도 단맛을 좋아하는 편이 아니어서 더 좋은 것 같은데요.', '그래서 아까 나도 한 입 먹어보고 진짜 깜짝 놀랐어.', '비서님 성함과 핸드폰 번호를 알려주시면 잘 안내해드리겠습니다.', '제 개인 비품 말고 입원할 때 가져와야 하는 것 있나요?', '의식은 있으신데, 말을 잘 못 하시고 눈도 못 뜨세요.', '지금 계획을 짜고 있는데, 출장이 처음이라 그런지 감이 안 잡힙니다.', '제품이 피부에 흡수되는 데 시간이 너무 오래 걸린다고 하더라고요.', '가운데에 큰 테이블을 기준으로 의자가 준비되어있습니다.', '응, 그건 들었어, 현장 공사 진행해도 될 것 같아?', '제가 영업부 팀장과 직원 2명과 박람회에 참여했습니다.', '네, 혹시 선호하시는 음식이 있으신가요?', '네, 10% 면세 가격으로 모든 상품을 구매하실 수 있습니다.', '이 멀리 있는 박물관에 뭐가 있다고 온 거야?', '마침 이번에 놀이공원에서 야간에 불꽃 축제를 한다고 들었어.', '최소한 5분 간격으로 두 장 이상 찍어주셔야 해요.', '신발 바닥 쪽에 치수가 쓰여 있는데 한번 확인해보시겠어요?', '4월에 진해에 갔었지. 거기가 명소이긴 하지.', '스파 시설 있는 커플룸으로 예약하고 싶은데요.', '하지만 당장 회의에서 결정할 사항이 많이 있는데요.', '풍경 사진 위주로 촬영할 카메라를 보려고요.', '닭갈비 코너는 매장 출구 쪽으로 자리를 옮겼어요.', '금액이 조금 더 들더라도 가이드 있는 상품으로 바꾸는 게 좋을 것 같아.', '뒤에 환전알림판을 보시면 환전금액이랑 수수료가 적혀있어요.', '오메가3랑 비타민D를 같이 복용해도 되나요?', '맞아, 처음에는 비 와서 가뭄이 해소되나 했는데 이제는 너무 싫어.', '제가 신용카드를 분실해서 분실 신고를 하려고 하는데요.', '그래, 엄마 포장해다 드리면 엄청나게 좋아하실 것 같네.', '조금 비싸도 좋으니 남아 있는 방이 뭐가 있나요?', '이번 여행에서 여기에 오는 걸 얼마나 기대했는지 몰라!', '미국 달러와 캐나다 달러 두 가지 중 어느 것으로 환전하시고 싶으신가요?', '이틀 동안 저희 챙겨주시고 하시느라 고생이 많으셨습니다.', '네, 임산부석도 아닌데 양보해 주셔서 감사합니다.', '제품소개도 하면서 판매해야 할 텐데, 아직 계획은 없네요.', '쌀국수 2개랑 모둠튀김 하나 배달 주문하고 싶습니다.', '집에 잘 들어가긴 했는데 과음으로 기억이 드문드문 나는 문제가 있네요.', '너도 알겠지만 나 항상 마지막에 결정하는 게 느리잖아.', '최근에 살이 많이 쪄서 검은색만 입고 싶어지네.', '둘 다 현금처럼 쓰이지만, 유럽에 가신다면 여행자 수표가 좀 더 경제적입니다.', '이런 상황에 실망하여 저는 노트북을 환불 처리하고 싶습니다.', '앞으로도 적어도 한 달은 계속 치료를 받으셔야 합니다.', '맞습니다, 개별포장이 하나씩 전부 되어있습니다.', '이건 짜장 가루인데요, 제가 찾는 건 카레 가루예요.', '잘못된 영수증이라니 저는 무슨 말씀인지 모르겠는데 설명 해주시겠어요?', '아니요, 총무팀과 의논하였고 아직 회계팀은 보지 못했습니다.', '갈비인데도 양념 맛이 하나도 느껴지지 않아서 별로야.', '고객님 정보에 핸드폰 번호를 입력하셨나요?', '그렇다고 이렇게 출장을 다녀오면서 혹처럼 달고 오면 어찌합니까.', '그런 스타일은 해 본 적이 없는데 괜찮을까요?', '그럼 비서실로 바로 연결해 드리겠습니다, 잠시만요.', '먼저 도서관에 회원으로 등록하시고 회원 카드를 발급받으셔야 해요.', '여기 투어 비용을 드릴게요.', '7/11 - 7/13에 저 혼자 묵으려고 하는데 혹시 방이 있나요?', '제가 어제 과음을 해서요. 죄송합니다. 교수님. 제 자리로 가겠습니다.', '그런 유명한 밴드의 공연을 해마다 볼 수 있다니 부럽네요.', '아직 다른 회사에는 보여드린 적 없는 단독 제안서입니다.', '당연하지, 그럼 먼저 자리 잡고 주문하러 가자.', '나는 졸업식 때 개근상을 한 번도 놓쳐본 적이 없어.', '아직 결정을 못 했어요, 혹시 이 메뉴는 무엇인가요?', '특별한 전시회가 벡스코에서 열린다고 들었는데 위치가 어디인가요?', '그러니까 말이야, 투자자들이 정말 실망이 클 것 같은 영화네.', '네, 최대한 꼼꼼히 읽어보고 업무에 적용해봐야겠어요.', '여기는 역사가 너무 넓어서 개찰구가 어디 있는지 모르겠어.', '완전히 너 취향이구나? 나는 한 번도 잘생겼다고 생각해본 적 없는데.', '현재 우리 브랜드 모델의 스캔들 때문인 건가요?', '경비실이 아니라 옆집에 맡기셨다네요, 내용을 잘못 적으셨대요.', '보호자님, 환자분 지금 주무시고 계시는데 불편하신 사항 있으시면 말씀해주세요.', '처음 한 달은 내 차를 타고 다음 달은 그 여자의 차를 타려고.', '오빠, 나 정말 감동이야, 깜짝 파티를 열어 줘서 정말 고마워.', '얼굴도 잘생기고 연기도 잘한다고 하지 않았어?', '만약에 제품을 5,000개 정도 발주를 했을 때 덤으로 더 안 주나요?', '이 담요는 고양이 털이 붙지 않는 재질로 되어 있는 건가요?', '아까 어떤 분이 검은색 전화기 하나를 맡겨두고 갔는데 한번 확인해보세요.', 'C 레벨 임원이 참여한다면 장소를 변경해야 합니다.', '혹시 점심에는 단품 메뉴로도 주문 가능한가요?', '네, 1번과 2번에 비해서 너무 간격이 넓은 것 같아요.', '코드 확인 완료되었고 여권 또는 신분증 보여주시면 됩니다.', '이전 회사에서 맡은 업무랑 전혀 다른 업무를 지원하셨는데 이유가 있나요?', '요즘 유행하는 그 혈관 파괴하는 조합 말하는 거 맞지?', '저는 연동하는 방법을 몰라서 못 했는데, 방법 알려주면 할게요.', '아까 주문한 파스타를 피자로 변경할 수 있나요?', '콜라만 주문하시는 건가요?', '단체 사진 촬영 한 번만 부탁해도 될까요?', '장소 불문, 시간 불문, 호탕하게 마시자!', '오늘 저녁에 학원으로 단체 주문하려고 하는데 배달되나요?', '그런데 주문을 늘리면 할인을 좀 해주실 수 있으세요?')\n",
            "(\"In the case of a parent's funeral, you can use up to 4 days and you should use them all at once.\", \"Cake is food as well, but why isn't it sold here?\", 'I am sorry, but could you give me your name and contact information?', 'Yeah, I need to know where this place is, so I can find it.', 'This is the rest stop where Lee Youngja introduced on TV.', 'I found out today too but the route for number 20 bus has been extended.', 'I want to purchase a season membership.', \"It's important that you try even if you're scared.\", \"It's to record any scratches or defects in the car, right?\", 'I would like to see the halfpipes snowboarding the most.', \"The next time I travel, I'll have to exchange some money for you.\", \"It'll cost extra on the highway, are you alright with that?\", \"Go down to the 1st basement and it'll be in the food stand.\", \"I agree with that, the spaces are so small and there aren't any armrests, which was uncomfortable.\", \"You can take a look inside the duty-free book that's in front of you.\", 'The HR Team used it in Meeting Room B for their monthly meeting yesterday.', 'Thank you. Is there a printer as well?', 'Yes. Our test will be from page 100 to page 250.', \"I'm really sorry, I'll exchange it with a new one.\", 'I lived in Italy from a young age so my accent may be different from other people.', 'You will lose a lot if you change to won now. Unless you need won urgently, I think it would be best just to keep your money.', \"I think we'd better go to the police station over there and ask.\", 'I was concerned about managing all these students so I feel quite relieved.', 'We should be the ones to thanks for being treated in such a good restaurant.', 'Sure, can I stamp for the 2 drinks on this coupon?', 'Yes, I got about that amount in the last company.', \"All the heroic movies are taking place in the theater, there's not much other choice.\", 'Yes. Our office is quite big, so I think we need to get around 3.', 'I want to pay, is this where I do it?', \"Even if I've watched it since I was a kid, baseball rules are really difficult.\", 'Okay, then I will deliver the same color and design but with the size of 90.', \"I haven't been able to use the stairs because my knees are sore since a few days ago.\", 'I spent years studying only program development, and I believe I will be needed in the development team that I applied for.', 'That’s 12,000 won in total, how would you like to pay?', 'Is the exclusive room an Ondol room?', 'Just in case, please make sure to enter it in today.', \"I'm sorry, there must be a mistake during cooking.\", \"That's right, the retail price is 120,000 won so you're getting a great deal.\", 'I think you will be able to do it, have you started exercising?', 'I heard this place is astonishingly beautiful in the summer.', 'I want you to leave it at 401, just across my house.', \"It's just liberal arts so I don't care, please help me.\", \"I don't think Asians will use it well.\", \"I don't like sweet flavor either, so I think it's better.\", 'So I took a bite earlier and I was really surprised.', \"If you tell me your secretary's name and cell phone number I will guide her well.\", 'Do I need to bring anything else besides my personal belongings?', \"She is conscious, but she can't speak well and is unable to open her eyes.\", \"I'm planning now, but it's my first time on a business trip so I can't figure it out.\", 'They said that it takes too long for the product to be absorbed into the skin.', 'There are chairs based on based on a large table in the middle.', 'Yes, I heard, do you think we can process the construction at the site?', 'I participated in the fair with the sales team manager and two other employees.', 'Okay, do you have any food you prefer?', 'Yes, you can purchase all products at 10% discounted prices.', 'What is there in this far-flung museum?', 'I just heard that the amusement park is having a fireworks festival at night.', 'You should take two or more pictures with at least 5 minutes gap.', 'The size is written on the bottom of the shoes, can you check it?', 'I went to Jinhae in April. I mean, that is the place famous for cherry blossoms.', \"I'd like to get a room for a couple with spa facility.\", 'But there are many things to decide at the meeting right now.', 'I want to look at cameras that will mainly take landscape photos.', 'The chicken ribs corner moved near the exit of the store.', 'Even if we have to pay extra, I think it will be better to switch the package to one with a guide.', 'If you look at the currency exchange behind me, the exchange amount and commission are written down.', 'Can I take Omega-3 and Vitamin D together?', 'I know, I liked it at first because it was a relief for the drought, but I hate it now.', 'I lost my credit card and want to report the loss.', \"Yeah, I'm sure mom would love it if we take it out for her.\", \"I don't mind if it is a little expensive, so is there any room left?\", \"You don't know how much I've been looking forward to coming here on this holiday!\", 'Which would you like to exchange to, U.S. dollars or Canadian dollars?', 'Thank you for taking care of us for two days.', \"Yes, thank you for giving up your sear when it's not even a seat intended for a pregnant woman.\", \"We need to introduce the products and sell them, but we don't have a plan yet.\", 'Please deliver 2 rice noodles and 1 assorted fries.', 'I went home just fine, but I also only have fragments of what happened the last day.', \"You know, I'm always slow to make the final decision.\", \"I've gained a lot of weight lately, so I just want to wear black.\", \"Both are used as cash, but if you're going to Europe, the check is a more economical choice.\", 'I am very disappointed in such a situation so I want a refund.', 'You need to continue receiving treatments for at least another month.', \"That's right, it's all wrapped up 1 by 1.\", \"This is black bean sauce powder, and what I'm looking for is curry powder.\", \"I don't understand what you mean by wrong receipts, please explain.\", 'No, I discussed with the secretary team and the accounting team has not yet seen the report.', \"It was ribs, but I couldn't taste any seasoning, so it sucked.\", 'Did you input your cellphone number into your account information?', 'But how could you bring him along back from your business trip, like baggage?', \"I haven't tried such a style, but do you think it would be okay?\", \"Okay, then I'll connect you directly to the secretary's office, just a second.\", 'You need to register as a member of this library first and get a membership card.', \"I'll pay for the tour fare.\", 'Is there a room available for one person from July 11th to the 13th?', \"I drank too much yesterday. I'm sorry, professor. I will go to my seat.\", 'I envy that you can see such a famous band performing every year.', \"It's a proposal only for you because I haven't shown it to other companies yet.\", \"You bet, then let's find a seat first and order.\", \"I've never missed a single perfect attendance award during my graduation ceremonies.\", \"I haven't decided yet, what's this menu?\", 'I heard that a special exhibition will be held at BEXCO but where is it?', \"That's what I'm talking about. I guess the investors will be very disappointed.\", 'Yes, I should read it as closely as possible and apply it to my work.', 'The station is too big here so I can’t find the ticket gate.', \"So he's your type? I never found him that attractive.\", 'Is it because of the scandal with our brand model?', \"It wasn't left in the security office, it was left with your next-door neighbor, he wrote it down wrong.\", 'The patient is sleeping now and let me know if there is any inconvenience.', 'Using mine for the first month, and then using hers for the next 1 month.', \"I'm so moved, thank you so much for opening a surprise birthday party for me.\", \"Didn't you say he is handsome and good at acting?\", 'Will you not give some extras when I order about 5,000 products?', 'Is this blanket made of non-cat hair material?', 'A person left a black phone here earlier, so have a look.', 'If they do, we have to change the conference venue.', 'Can I order a single dish for lunch?', \"Yes, it's too wide compared to the number 1 and 2.\", 'We have checked your code, and you can show me either your passport or your ID.', 'Do you have any reason why you applied for a completely different job from your previous job?', \"You're talking about the blood vessel-destroying combination that's going around recently, right?\", \"I couldn't do it because I didn't know how to do it, but I'll do it if you teach me how.\", 'Can I change my order from pasta to pizza?', 'Are you ordering just a coke?', 'Can I ask for a group photo, please?', \"Regardless of location, regardless of time, let's have a mighty drink!\", 'I want to order for a group for tonight to the institute, can I get a delivery?', 'But if I increase the order, can you give me some discount?')\n",
            "128\n",
            "128\n",
            "tensor([[32121, 38219, 29578, 32645, 39226, 32602, 28705, 28781, 29415, 32440,\n",
            "         32563, 49875, 34736, 33098, 32440, 29136, 44233, 42496, 28723,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2],\n",
            "        [36378, 30056, 29599, 38965, 34017, 28725, 34508, 35244, 32925, 32168,\n",
            "         53160, 32346, 40180, 28804,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2]])\n",
            "tensor([[    1,   560,   272,  1222,   302,   264,  2564, 28742, 28713, 18979,\n",
            "         28725,   368,   541,   938,   582,   298, 28705, 28781,  2202,   304,\n",
            "           368,  1023,   938,   706,   544,   438,  2327, 28723,     2,     2,\n",
            "             2,     2],\n",
            "        [    1,   334,   621,   349,  2887,   390,  1162, 28725,   562,  2079,\n",
            "          3157, 28742, 28707,   378,  3910,  1236, 28804,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2]])\n",
            "torch.Size([128, 27])\n",
            "torch.Size([128, 32])\n",
            "tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "        28723,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2])\n",
            "<|startoftext|> In the case of a parent's funeral, you can use up to 4 days and you should use them all at once.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "tensor([    1,   315,  1419,   575,  3154,  1368,   562,   272,  7103,   354,\n",
            "         1474, 28705, 28750, 28734,  1579,   659,   750,  8766, 28723,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2])\n",
            "tensor([  315,  1419,   575,  3154,  1368,   562,   272,  7103,   354,  1474,\n",
            "        28705, 28750, 28734,  1579,   659,   750,  8766, 28723,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0][\"kor\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "h89kg-zp2OZ6",
        "outputId": "06c97bc5-1914-4213-d956-9b405416988f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'이번 신제품 출시에 대한 시장의 반응은 어떤가요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"eos 토큰 인덱스:\",tokenizer.eos_token_id)\n",
        "print(\"pad 토큰 인덱스:\",tokenizer.pad_token_id)\n",
        "print(\"sos 토큰 인덱스:\",tokenizer.bos_token_id)\n",
        "print(\"unk 토큰 인덱스:\",tokenizer.unk_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do4uSUM_tpCT",
        "outputId": "1aebd5d9-6b2c-442f-86fa-9794893e99dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eos 토큰 인덱스: 2\n",
            "pad 토큰 인덱스: 0\n",
            "sos 토큰 인덱스: 1\n",
            "unk 토큰 인덱스: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 한글이 포함된 토큰만 필터링\n",
        "kor_vocab = {token: idx for token, idx in tokenizer.get_vocab().items() if re.search(r'[ㄱ-ㅎㅏ-ㅣ가-힣]', token)}\n",
        "\n",
        "# 영어 토큰만 필터링 (알파벳 포함 여부 확인)\n",
        "eng_vocab = {token: idx for token, idx in tokenizer.get_vocab().items() if re.search(r'[a-zA-Z]', token)}\n",
        "\n",
        "print(\"한국어 토큰 개수:\",len(kor_vocab))\n",
        "print(\"영어 토큰 개수:\",len(eng_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEbEFc1bZPVv",
        "outputId": "cf385429-2305-445a-fb96-6c82fcebe0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 토큰 개수: 12191\n",
            "영어 토큰 개수: 25942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_collate_fn(pad_index):\n",
        "    def collate_fn(batch):\n",
        "        # 이미 tensor라면 그대로 사용\n",
        "        batch_kor_ids = [example[\"kor_ids\"].clone().detach() if isinstance(example[\"kor_ids\"], torch.Tensor) else torch.tensor(example[\"kor_ids\"], dtype=torch.long) for example in batch]\n",
        "        batch_eng_ids = [example[\"eng_ids\"].clone().detach() if isinstance(example[\"eng_ids\"], torch.Tensor) else torch.tensor(example[\"eng_ids\"], dtype=torch.long) for example in batch]\n",
        "\n",
        "        # ✅ 리스트 → 하나의 `tensor(batch_size, seq_len)` 변환\n",
        "        batch_kor_ids = torch.stack(batch_kor_ids)\n",
        "        batch_eng_ids = torch.stack(batch_eng_ids)\n",
        "\n",
        "        return {\n",
        "            \"kor_ids\": batch_kor_ids,  # ✅ (batch_size, seq_len)\n",
        "            \"eng_ids\": batch_eng_ids   # ✅ (batch_size, seq_len)\n",
        "        }\n",
        "\n",
        "    return collate_fn\n",
        "\n",
        "batch_size = 128\n",
        "collate_fn = get_collate_fn(pad_index=0)\n",
        "\n",
        "def get_data_loader(dataset, batch_size, shuffle = False):\n",
        "  data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "  return data_loader\n",
        "\n",
        "train_loader = get_data_loader(train_data, batch_size=128, shuffle=False)\n",
        "val_loader = get_data_loader(val_data, batch_size=128, shuffle=False)\n",
        "test_loader = get_data_loader(test_data, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "RypumjeH1L91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data[0][\"kor_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYW_iEgt7pfV",
        "outputId": "e19a7706-ed3c-4126-da65-7710e00c2287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_loader:\n",
        "  print(batch[\"kor_ids\"].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovGHircR38nT",
        "outputId": "e3710b6d-233a-4aac-84c0-04fa1221ef06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "# seed 고정\n",
        "def seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(42)\n",
        "\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, text, tokenizer, vocab_size, max_length = max_len, embedding_dim=1000):\n",
        "#         self.text = text\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.text)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#       text = self.text[idx]\n",
        "\n",
        "#       encoding = self.tokenizer(\n",
        "#                                 text,\n",
        "#                                 padding=\"max_length\",\n",
        "#                                 max_length=self.max_length,\n",
        "#                                 truncation= True,\n",
        "#                                 return_tensors=\"pt\"\n",
        "#                                 )\n",
        "#       input_idx = encoding[\"input_ids\"]\n",
        "#       embedded_inputs = self.emb(input_idx)\n",
        "\n",
        "\n",
        "#       return embedded_inputs.squeeze(0) # (1, max_length, emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # 패딩을 오른쪽으로 설정\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "# # pad token index 설정\n",
        "# tokenizer.pad_token_id = 0  # 패딩 토큰의 인덱스 설정\n",
        "\n",
        "\n",
        "# # source 데이터 분리\n",
        "# train_src = data_kor_list[:89999]\n",
        "# val_src = data_kor_list[90000:94999]\n",
        "# test_src = data_kor_list[9500:]\n",
        "\n",
        "# # target 데이터 분리\n",
        "# train_target = data_eng_list[:89999]\n",
        "# val_target = data_eng_list[90000:94999]\n",
        "# test_target = data_eng_list[95000:]\n",
        "\n",
        "\n",
        "# # 데이터셋 생성\n",
        "# train_src_dataset = CustomDataset(text = train_src, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "# val_src_dataset = CustomDataset(text = val_src, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "# test_src_dataset = CustomDataset(text = test_src, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "\n",
        "# train_input_data = train_data\n",
        "\n",
        "\n",
        "# # train dataloader\n",
        "# train_src_dataloader = DataLoader(train_src_dataset, batch_size=128, shuffle=False)\n",
        "# val_src_dataloader = DataLoader(val_src_dataset, batch_size=128, shuffle=False)\n",
        "# test_src_dataloader = DataLoader(test_src_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "iqUK97SxcCRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target data\n",
        "# train_target_dataset = CustomDataset(text = train_target, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "# val_target_dataset = CustomDataset(text = val_target, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "# test_target_dataset = CustomDataset(text = test_target, tokenizer = tokenizer,  vocab_size = vocab_size)\n",
        "\n",
        "# train_target_dataloader = DataLoader(train_target_dataset, batch_size=128, shuffle=False)\n",
        "# val_target_dataloader = DataLoader(val_target_dataset, batch_size=128, shuffle=False)\n",
        "# test_target_dataloader = DataLoader(test_target_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "juTX9wpT834g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src length, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src length, batch size, embedding dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs = [src length, batch size, hidden dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # outputs are always from the top hidden layer\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        # context = [n layers, batch size, hidden dim]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch size, embedding dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output = [seq length, batch size, hidden dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
        "        # output = [1, batch size, hidden dim]\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        # cell = [n layers, batch size, hidden dim]\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        assert (\n",
        "            encoder.hidden_dim == decoder.hidden_dim\n",
        "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert (\n",
        "            encoder.n_layers == decoder.n_layers\n",
        "        ), \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio):\n",
        "        # src = [src length, batch size]\n",
        "        # trg = [trg length, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_length = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "        # input = [batch size]\n",
        "        for t in range(1, trg_length):\n",
        "            # insert input token embedding, previous hidden and previous cell states\n",
        "            # receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            # output = [batch size, output dim]\n",
        "            # hidden = [n layers, batch size, hidden dim]\n",
        "            # cell = [n layers, batch size, hidden dim]\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            # input = [batch size]\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "MdFsQHM8CVaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(tokenizer.get_vocab().items())\n",
        "output_dim = len(tokenizer.get_vocab().items())\n",
        "encoder_embedding_dim = 256\n",
        "decoder_embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    input_dim,\n",
        "    encoder_embedding_dim,\n",
        "    hidden_dim,\n",
        "    n_layers,\n",
        "    encoder_dropout,\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    output_dim,\n",
        "    decoder_embedding_dim,\n",
        "    hidden_dim,\n",
        "    n_layers,\n",
        "    decoder_dropout,\n",
        ")\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "FUcKi5UOkROf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "5be34cfa-1e89-4156-c6f4-997b39ee29f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-99f1da3cd47e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "Ith-9WYjvXFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_dim,\"input_dim\")\n",
        "print(output_dim,\"output_dim\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJWqQpv-VVdk",
        "outputId": "09c9c4d6-8c0a-4d48-d1be-b487805722ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64000 input_dim\n",
            "64000 output_dim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJLaDAPy1bh",
        "outputId": "42a9b690-7caa-41c4-82f6-33301b999c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 30,426,710 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_index = tokenizer.pad_token_id\n",
        "print(pad_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKtNg9hj6xks",
        "outputId": "40aebbd0-6535-411d-9577-1cf1bd848d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index= pad_index)\n",
        "\n",
        "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        print(batch[\"kor_ids\"].dtype)  # float32/int64/long 확인\n",
        "        print(batch[\"kor_ids\"].min(), batch[\"kor_ids\"].max())  # 범위 체크\n",
        "        print(batch[\"eng_ids\"].min(), batch[\"eng_ids\"].max())  # 범위 체크\n",
        "        print(batch[\"kor_ids\"].device)\n",
        "        src = batch[\"kor_ids\"].to(device).long()\n",
        "        print(src.dtype)\n",
        "        trg = batch[\"eng_ids\"].to(device).long()\n",
        "        print(src.shape)\n",
        "        print(trg.shape)\n",
        "        # src = [src length, batch size]\n",
        "        # trg = [trg length, batch size]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio)\n",
        "        # output = [trg length, batch size, trg vocab size]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        print(output.shape)\n",
        "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(trg length - 1) * batch size]\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "W4m2g9OY0MSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fn(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            src = batch[\"kor_ids\"].to(device).long()\n",
        "            trg = batch[\"eng_ids\"].to(device).long()\n",
        "            print(src.shape)\n",
        "            print(trg.shape)\n",
        "            # src = [src length, batch size]\n",
        "            # trg = [trg length, batch size]\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "            # output = [trg length, batch size, trg vocab size]\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            print(output.shape)\n",
        "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
        "            trg = trg[1:].view(-1)\n",
        "            # trg = [(trg length - 1) * batch size]\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "MDhq-fjh4xJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "n_epochs = 10\n",
        "clip = 1.0\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    train_loss = train_fn(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        clip,\n",
        "        teacher_forcing_ratio,\n",
        "        device,\n",
        "    )\n",
        "    valid_loss = evaluate_fn(\n",
        "        model,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        device,\n",
        "    )\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "17f501bf3487414baf8f64e14e7890a1",
            "824265d9236545f7a0503236ee7d1d27",
            "8196177a52d445bebcc7d4f3ff8e0703",
            "7aadcf8dc82144c5a81b10f617b8686a",
            "016fb6b5b73f4ecf84ecfdd5d228d3b7",
            "2501fb2b5a2843d2858f32f3ec261a9e",
            "7b120ecf7f3f4535a954b2e30db4d980",
            "1a22ad9534c040a28c10522d04f728da",
            "351700ee15094fb1835fc3c7f896e35e",
            "55eab2df550b46a8bb215cee6ed37967",
            "026d9483f5de471cbd800e19dc658892"
          ]
        },
        "id": "st_koGvh5TKl",
        "outputId": "f3206e92-74b0-4a7f-e4a3-a48ad6f62948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17f501bf3487414baf8f64e14e7890a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n",
            "tensor(0) tensor(62733)\n",
            "tensor(0) tensor(28808)\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f10d35241354>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_loss = train_fn(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-138ee4d4c6a5>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 범위 체크\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kor_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kor_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLgDWVe15Xm1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}